{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Data Extractor\n",
    "Last code chunk is what worked. I'll clean up this notebook this week and refine the data extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import certifi\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull articles with sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "\n",
    "def get_jsonparsed_data(url):\n",
    "    \"\"\"\n",
    "    Fetch JSON data from the given URL.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL to fetch data from.\n",
    "\n",
    "    Returns:\n",
    "    dict: The parsed JSON data.\n",
    "    \"\"\"\n",
    "    response = urlopen(url, cafile=certifi.where())\n",
    "    data = response.read().decode(\"utf-8\")\n",
    "    return json.loads(data)\n",
    "\n",
    "def fetch_stock_news_sentiments(api_key, page=0):\n",
    "    \"\"\"\n",
    "    Fetch stock news sentiments from Financial Modeling Prep API.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    page (int): Page number for pagination. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "    list: The parsed JSON data containing news sentiments.\n",
    "    \"\"\"\n",
    "    url = f\"https://financialmodelingprep.com/api/v4/stock-news-sentiments-rss-feed?page={page}&apikey={api_key}\"\n",
    "    return get_jsonparsed_data(url)\n",
    "\n",
    "def save_news_sentiments_to_file(all_news_sentiments, filename):\n",
    "    \"\"\"\n",
    "    Save all news sentiments to a single JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    all_news_sentiments (list): The list of all news sentiments to save.\n",
    "    filename (str): The name of the file to save the news sentiments.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(all_news_sentiments, file, indent=4)\n",
    "    print(f\"All news sentiments saved to {filename}\")\n",
    "\n",
    "def fetch_and_save_all_news(api_key, max_pages=5):\n",
    "    \"\"\"\n",
    "    Fetch and save news articles into a single JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    max_pages (int): Maximum number of pages to fetch. Default is 5.\n",
    "    \"\"\"\n",
    "    all_news_sentiments = []\n",
    "    for page in range(max_pages):\n",
    "        print(f\"Fetching page {page}...\")\n",
    "        news_sentiments = fetch_stock_news_sentiments(api_key, page)\n",
    "        if not news_sentiments:\n",
    "            break  # Exit the loop if no more news is returned\n",
    "        all_news_sentiments.extend(news_sentiments)\n",
    "        time.sleep(.1)  # Sleep to avoid hitting API rate limits\n",
    "\n",
    "    save_news_sentiments_to_file(all_news_sentiments, 'all_news_sentiments.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j2/skjc081x2h5cd717r89c0q480000gn/T/ipykernel_80729/1768911447.py:16: DeprecationWarning: cafile, capath and cadefault are deprecated, use a custom context instead.\n",
      "  response = urlopen(url, cafile=certifi.where())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1...\n",
      "Fetching page 2...\n",
      "Fetching page 3...\n",
      "Fetching page 4...\n",
      "Fetching page 5...\n",
      "Fetching page 6...\n",
      "Fetching page 7...\n",
      "Fetching page 8...\n",
      "Fetching page 9...\n",
      "Fetching page 10...\n",
      "Fetching page 11...\n",
      "Fetching page 12...\n",
      "Fetching page 13...\n",
      "Fetching page 14...\n",
      "Fetching page 15...\n",
      "Fetching page 16...\n",
      "Fetching page 17...\n",
      "Fetching page 18...\n",
      "Fetching page 19...\n",
      "Fetching page 20...\n",
      "Fetching page 21...\n",
      "Fetching page 22...\n",
      "Fetching page 23...\n",
      "Fetching page 24...\n",
      "Fetching page 25...\n",
      "Fetching page 26...\n",
      "Fetching page 27...\n",
      "Fetching page 28...\n",
      "Fetching page 29...\n",
      "Fetching page 30...\n",
      "Fetching page 31...\n",
      "Fetching page 32...\n",
      "Fetching page 33...\n",
      "Fetching page 34...\n",
      "Fetching page 35...\n",
      "Fetching page 36...\n",
      "Fetching page 37...\n",
      "Fetching page 38...\n",
      "Fetching page 39...\n",
      "Fetching page 40...\n",
      "Fetching page 41...\n",
      "Fetching page 42...\n",
      "Fetching page 43...\n",
      "Fetching page 44...\n",
      "Fetching page 45...\n",
      "Fetching page 46...\n",
      "Fetching page 47...\n",
      "Fetching page 48...\n",
      "Fetching page 49...\n",
      "Fetching page 50...\n",
      "Fetching page 51...\n",
      "Fetching page 52...\n",
      "Fetching page 53...\n",
      "Fetching page 54...\n",
      "Fetching page 55...\n",
      "Fetching page 56...\n",
      "Fetching page 57...\n",
      "Fetching page 58...\n",
      "Fetching page 59...\n",
      "Fetching page 60...\n",
      "Fetching page 61...\n",
      "Fetching page 62...\n",
      "Fetching page 63...\n",
      "Fetching page 64...\n",
      "Fetching page 65...\n",
      "Fetching page 66...\n",
      "Fetching page 67...\n",
      "Fetching page 68...\n",
      "Fetching page 69...\n",
      "Fetching page 70...\n",
      "Fetching page 71...\n",
      "Fetching page 72...\n",
      "Fetching page 73...\n",
      "Fetching page 74...\n",
      "Fetching page 75...\n",
      "Fetching page 76...\n",
      "Fetching page 77...\n",
      "Fetching page 78...\n",
      "Fetching page 79...\n",
      "Fetching page 80...\n",
      "Fetching page 81...\n",
      "Fetching page 82...\n",
      "Fetching page 83...\n",
      "Fetching page 84...\n",
      "Fetching page 85...\n",
      "Fetching page 86...\n",
      "Fetching page 87...\n",
      "Fetching page 88...\n",
      "Fetching page 89...\n",
      "Fetching page 90...\n",
      "Fetching page 91...\n",
      "Fetching page 92...\n",
      "Fetching page 93...\n",
      "Fetching page 94...\n",
      "Fetching page 95...\n",
      "Fetching page 96...\n",
      "Fetching page 97...\n",
      "Fetching page 98...\n",
      "Fetching page 99...\n",
      "All news sentiments saved to all_news_sentiments.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "api_key = '28a3eef526c43ab5888ab02222aada18'\n",
    "\n",
    "# Specify the number of pages you want to fetch\n",
    "max_pages = 100\n",
    "\n",
    "fetch_and_save_all_news(api_key, max_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  symbol             publishedDate  \\\n",
      "0     BA  2024-07-01T12:16:08.000Z   \n",
      "1   APOG  2024-07-01T12:15:29.000Z   \n",
      "2    QSR  2024-07-01T12:15:00.000Z   \n",
      "3    TDC  2024-07-01T12:15:00.000Z   \n",
      "4   VKTX  2024-07-01T12:15:00.000Z   \n",
      "\n",
      "                                               title  \\\n",
      "0  Boeing Agrees To Reacquire Spirit AeroSytems f...   \n",
      "1                 New Strong Buy Stocks for July 1st   \n",
      "2  Tims China AnnouncesÂ Significant Financing fro...   \n",
      "3  TDC LAWSUIT NOTIFICATION: BFA Law Notifies Ter...   \n",
      "4  Viking Therapeutics Stock Is Up by 154% Since ...   \n",
      "\n",
      "                                               image               site  \\\n",
      "0  https://cdn.snapi.dev/images/v1/g/j/ba16-24797...   investopedia.com   \n",
      "1  https://cdn.snapi.dev/images/v1/1/d/building-m...          zacks.com   \n",
      "2  https://cdn.snapi.dev/images/v1/m/g/press19-25...  globenewswire.com   \n",
      "3  https://cdn.snapi.dev/images/v1/y/6/press11-25...  globenewswire.com   \n",
      "4  https://cdn.snapi.dev/images/v1/j/i/biotech9-2...           fool.com   \n",
      "\n",
      "                                                text  \\\n",
      "0  Boeing (BA) announced a long-awaited deal Mond...   \n",
      "1  EAT, APOG, DAKT, M and HES have been added to ...   \n",
      "2  SHANGHAI, China and NEW YORK, July 01, 2024 (G...   \n",
      "3  NEW YORK, July 01, 2024 (GLOBE NEWSWIRE) -- To...   \n",
      "4  Viking Therapeutics stock is flying high thank...   \n",
      "\n",
      "                                                 url sentiment  sentimentScore  \n",
      "0  https://www.investopedia.com/boeing-agrees-to-...   Neutral          -0.072  \n",
      "1  https://www.zacks.com/commentary/2294765/new-s...  Positive           0.400  \n",
      "2  https://www.globenewswire.com/news-release/202...  Positive           0.200  \n",
      "3  https://www.globenewswire.com/news-release/202...  Positive           0.312  \n",
      "4  https://www.fool.com/investing/2024/07/01/viki...  Positive           0.200  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def json_to_dataframe(json_file):\n",
    "    \"\"\"\n",
    "    Convert JSON file to a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    json_file (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame containing the JSON data.\n",
    "    \"\"\"\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Create a DataFrame from the list of articles\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "json_file = 'all_news_sentiments.json'\n",
    "df = json_to_dataframe(json_file)\n",
    "print(df.head())  # Print the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to CSV and Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_news_sentiments.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('all_news_sentiments.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull Articles with sentiment scores, adjust date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_jsonparsed_data(url):\n",
    "    \"\"\"\n",
    "    Fetch JSON data from the given URL.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL to fetch data from.\n",
    "\n",
    "    Returns:\n",
    "    dict: The parsed JSON data.\n",
    "    \"\"\"\n",
    "    response = urlopen(url, cafile=certifi.where())\n",
    "    data = response.read().decode(\"utf-8\")\n",
    "    return json.loads(data)\n",
    "\n",
    "def fetch_stock_news_sentiments(api_key, ticker, date_from, date_to, limit=50):\n",
    "    \"\"\"\n",
    "    Fetch stock news sentiments from Financial Modeling Prep API.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    ticker (str): The stock ticker symbol (e.g., 'AAPL' for Apple).\n",
    "    date_from (str): Start date for fetching articles (YYYY-MM-DD).\n",
    "    date_to (str): End date for fetching articles (YYYY-MM-DD).\n",
    "    limit (int): Number of articles to fetch per request. Default is 50.\n",
    "\n",
    "    Returns:\n",
    "    list: The parsed JSON data containing news sentiments.\n",
    "    \"\"\"\n",
    "    url = f\"https://financialmodelingprep.com/api/v4/stock-news-sentiments-rss-feed?symbol={ticker}&from={date_from}&to={date_to}&apikey={api_key}&limit={limit}\"\n",
    "    return get_jsonparsed_data(url)\n",
    "\n",
    "def save_news_sentiments_to_file(all_news_sentiments, filename):\n",
    "    \"\"\"\n",
    "    Save all news sentiments to a single JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    all_news_sentiments (list): The list of all news sentiments to save.\n",
    "    filename (str): The name of the file to save the news sentiments.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(all_news_sentiments, file, indent=4)\n",
    "    print(f\"All news sentiments saved to {filename}\")\n",
    "\n",
    "def fetch_and_save_all_news(api_key, tickers, start_date, end_date, interval_days=7, limit=10):\n",
    "    \"\"\"\n",
    "    Fetch and save news articles for a list of tickers into a single JSON file over a specified timeframe.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    tickers (list): A list of ticker symbols.\n",
    "    start_date (str): Start date for fetching articles (YYYY-MM-DD).\n",
    "    end_date (str): End date for fetching articles (YYYY-MM-DD).\n",
    "    interval_days (int): Number of days in each interval for fetching articles. Default is 7.\n",
    "    limit (int): Number of articles to fetch per request. Default is 50.\n",
    "    \"\"\"\n",
    "    all_news_sentiments = []\n",
    "    current_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    while current_date < end_date:\n",
    "        next_date = current_date + timedelta(days=interval_days)\n",
    "        for ticker in tickers:\n",
    "            print(f\"Fetching news for {ticker} from {current_date.strftime('%Y-%m-%d')} to {next_date.strftime('%Y-%m-%d')}...\")\n",
    "            news_sentiments = fetch_stock_news_sentiments(api_key, ticker, current_date.strftime('%Y-%m-%d'), next_date.strftime('%Y-%m-%d'), limit)\n",
    "            if news_sentiments:\n",
    "                all_news_sentiments.extend(news_sentiments)\n",
    "            time.sleep(1)  # Sleep to avoid hitting API rate limits\n",
    "        current_date = next_date\n",
    "\n",
    "    save_news_sentiments_to_file(all_news_sentiments, f'{savename}.json')\n",
    "\n",
    "def fetch_sp500_tickers():\n",
    "    \"\"\"\n",
    "    Fetch the list of S&P 500 ticker symbols from Wikipedia.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of S&P 500 ticker symbols.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data from Wikipedia: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', {'id': 'constituents'})\n",
    "    tickers = []\n",
    "\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        ticker = row.find_all('td')[0].text.strip()\n",
    "        tickers.append(ticker)\n",
    "\n",
    "    return tickers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "api_key = '28a3eef526c43ab5888ab02222aada18'\n",
    "sp500_tickers = fetch_sp500_tickers()\n",
    "sp500_tickers = sp500_tickers[:1]  # Fetch news for the first n tickers\n",
    "savename = 'test'\n",
    "\n",
    "# Specify the date range\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-02-28'\n",
    "\n",
    "if sp500_tickers:\n",
    "    fetch_and_save_all_news(api_key, ['AAPL'], start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def json_to_dataframe(json_file):\n",
    "    \"\"\"\n",
    "    Convert JSON file to a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    json_file (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame containing the JSON data.\n",
    "    \"\"\"\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Create a DataFrame from the list of articles\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "json_file = 'test.json'\n",
    "df = json_to_dataframe(json_file)\n",
    "print(df.head())  # Print the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to CSV and Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{savename}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('all_news_sentiments.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Stock news Extractor\n",
    "For extracting general stock news. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_stock_news(api_key, ticker, date_from, date_to, limit=50):\n",
    "    \"\"\"\n",
    "    Fetch stock news from Financial Modeling Prep API.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    ticker (str): The stock ticker symbol (e.g., 'AAPL' for Apple).\n",
    "    date_from (str): Start date for fetching articles (YYYY-MM-DD).\n",
    "    date_to (str): End date for fetching articles (YYYY-MM-DD).\n",
    "    limit (int): Number of articles to fetch per request. Default is 50.\n",
    "\n",
    "    Returns:\n",
    "    list: The parsed JSON data containing news articles.\n",
    "    \"\"\"\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/stock_news?tickers={ticker}&from={date_from}&to={date_to}&limit={limit}&apikey={api_key}\"\n",
    "    return get_jsonparsed_data(url)\n",
    "\n",
    "def save_news_to_file(all_news, filename):\n",
    "    \"\"\"\n",
    "    Save all news articles to a single JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    all_news (list): The list of all news articles to save.\n",
    "    filename (str): The name of the file to save the news articles.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(all_news, file, indent=4)\n",
    "    print(f\"All news articles saved to {filename}\")\n",
    "\n",
    "def fetch_and_save_all_news(api_key, tickers, start_date, end_date, interval_days=7, max_articles_per_interval=50):\n",
    "    \"\"\"\n",
    "    Fetch and save news articles for a list of tickers into a single JSON file over a specified timeframe.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    tickers (list): A list of ticker symbols.\n",
    "    start_date (str): Start date for fetching articles (YYYY-MM-DD).\n",
    "    end_date (str): End date for fetching articles (YYYY-MM-DD).\n",
    "    interval_days (int): Number of days in each interval for fetching articles. Default is 7.\n",
    "    max_articles_per_interval (int): Maximum number of articles to fetch per interval. Default is 50.\n",
    "    \"\"\"\n",
    "    all_news = []\n",
    "    current_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    while current_date < end_date:\n",
    "        next_date = current_date + timedelta(days=interval_days)\n",
    "        for ticker in tickers:\n",
    "            print(f\"Fetching news for {ticker} from {current_date.strftime('%Y-%m-%d')} to {next_date.strftime('%Y-%m-%d')}...\")\n",
    "            random_start_date = current_date + timedelta(days=random.randint(0, interval_days - 1))\n",
    "            random_end_date = random_start_date + timedelta(days=random.randint(1, interval_days - (random_start_date - current_date).days))\n",
    "            limit = random.randint(1, max_articles_per_interval)\n",
    "            news_articles = fetch_stock_news(api_key, ticker, random_start_date.strftime('%Y-%m-%d'), random_end_date.strftime('%Y-%m-%d'), limit)\n",
    "            if news_articles:\n",
    "                # Filter articles to include only those for the specified ticker\n",
    "                filtered_news_articles = [article for article in news_articles if ticker in article.get('symbol', '')]\n",
    "                all_news.extend(filtered_news_articles)\n",
    "            time.sleep(1)  # Sleep to avoid hitting API rate limits\n",
    "        current_date = next_date\n",
    "\n",
    "    save_news_to_file(all_news, 'all_stock_news.json')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "api_key = '28a3eef526c43ab5888ab02222aada18'\n",
    "sp500_tickers = fetch_sp500_tickers()\n",
    "\n",
    "# Limit the tickers for testing\n",
    "limited_tickers = sp500_tickers #[:1]\n",
    "\n",
    "# Specify the date range\n",
    "start_date = '2023-10-01'\n",
    "end_date = '2023-12-31'\n",
    "\n",
    "if limited_tickers:\n",
    "    fetch_and_save_all_news(api_key, limited_tickers, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def json_to_dataframe(json_file):\n",
    "    \"\"\"\n",
    "    Convert JSON file to a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    json_file (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame containing the JSON data.\n",
    "    \"\"\"\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Create a DataFrame from the list of articles\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "json_file = 'all_stock_news.json'\n",
    "df = json_to_dataframe(json_file)\n",
    "print(df.head())  # Print the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to CSV and Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_news.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('all_news.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull Stock Price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_historical_stock_price(api_key, ticker, date_from, date_to):\n",
    "    \"\"\"\n",
    "    Fetch historical stock price data from Financial Modeling Prep API.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    ticker (str): The stock ticker symbol (e.g., 'AAPL' for Apple).\n",
    "    date_from (str): Start date for fetching data (YYYY-MM-DD).\n",
    "    date_to (str): End date for fetching data (YYYY-MM-DD).\n",
    "\n",
    "    Returns:\n",
    "    list: The parsed JSON data containing historical stock prices.\n",
    "    \"\"\"\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/historical-price-full/{ticker}?from={date_from}&to={date_to}&apikey={api_key}\"\n",
    "    return get_jsonparsed_data(url)\n",
    "\n",
    "def save_historical_prices_to_file(all_prices, filename):\n",
    "    \"\"\"\n",
    "    Save all historical stock prices to a single JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    all_prices (list): The list of all historical stock prices to save.\n",
    "    filename (str): The name of the file to save the historical stock prices.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(all_prices, file, indent=4)\n",
    "    print(f\"All historical stock prices saved to {filename}\")\n",
    "\n",
    "def fetch_and_save_all_historical_prices(api_key, tickers, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch and save historical stock prices for a list of tickers into a single JSON file over a specified timeframe.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    tickers (list): A list of ticker symbols.\n",
    "    start_date (str): Start date for fetching data (YYYY-MM-DD).\n",
    "    end_date (str): End date for fetching data (YYYY-MM-DD).\n",
    "    \"\"\"\n",
    "    all_historical_prices = []\n",
    "    for ticker in tickers:\n",
    "        print(f\"Fetching historical prices for {ticker} from {start_date} to {end_date}...\")\n",
    "        historical_prices = fetch_historical_stock_price(api_key, ticker, start_date, end_date)\n",
    "        if historical_prices:\n",
    "            all_historical_prices.append({ticker: historical_prices})\n",
    "        time.sleep(1)  # Sleep to avoid hitting API rate limits\n",
    "\n",
    "    save_historical_prices_to_file(all_historical_prices, 'all_historical_prices.json')\n",
    "\n",
    "def fetch_sp500_tickers():\n",
    "    \"\"\"\n",
    "    Fetch the list of S&P 500 ticker symbols from Wikipedia.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of S&P 500 ticker symbols.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data from Wikipedia: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', {'id': 'constituents'})\n",
    "    tickers = []\n",
    "\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        ticker = row.find_all('td')[0].text.strip()\n",
    "        tickers.append(ticker)\n",
    "\n",
    "    return tickers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "api_key = '28a3eef526c43ab5888ab02222aada18'\n",
    "sp500_tickers = fetch_sp500_tickers()\n",
    "\n",
    "# Limit to the first ticker (MMM)\n",
    "limited_tickers = sp500_tickers #[:1]\n",
    "\n",
    "# Specify the date range\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-12-31'\n",
    "\n",
    "if limited_tickers:\n",
    "    fetch_and_save_all_historical_prices(api_key, limited_tickers, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert prices json data to pandas dataframe, csv, parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def json_to_dataframe(json_file):\n",
    "    \"\"\"\n",
    "    Convert JSON file to a pandas DataFrame in tidy data format.\n",
    "\n",
    "    Parameters:\n",
    "    json_file (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame in tidy data format.\n",
    "    \"\"\"\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    records = []\n",
    "    for ticker_data in data:\n",
    "        for ticker, content in ticker_data.items():\n",
    "            historical_data = content[\"historical\"]\n",
    "            for price in historical_data:\n",
    "                price['ticker'] = ticker\n",
    "                records.append(price)\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "def save_dataframe_to_csv(df, csv_file):\n",
    "    \"\"\"\n",
    "    Save DataFrame to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The pandas DataFrame to save.\n",
    "    csv_file (str): The name of the CSV file.\n",
    "    \"\"\"\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"Data saved to {csv_file}\")\n",
    "\n",
    "def save_dataframe_to_parquet(df, parquet_file):\n",
    "    \"\"\"\n",
    "    Save DataFrame to a Parquet file.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The pandas DataFrame to save.\n",
    "    parquet_file (str): The name of the Parquet file.\n",
    "    \"\"\"\n",
    "    df.to_parquet(parquet_file, index=False)\n",
    "    print(f\"Data saved to {parquet_file}\")\n",
    "\n",
    "# Example usage\n",
    "json_file = 'all_historical_prices.json'\n",
    "csv_file = 'all_historical_prices.csv'\n",
    "parquet_file = 'all_historical_prices.parquet'\n",
    "\n",
    "# Convert JSON to DataFrame\n",
    "df = json_to_dataframe(json_file)\n",
    "\n",
    "# Check if all columns are retained\n",
    "print(\"Columns in DataFrame:\", df.columns)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "save_dataframe_to_csv(df, csv_file)\n",
    "\n",
    "# Save DataFrame to Parquet\n",
    "save_dataframe_to_parquet(df, parquet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py31",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
