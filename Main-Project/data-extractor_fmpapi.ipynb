{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Data Extractor\n",
    "Last code chunk is what worked. I'll clean up this notebook this week and refine the data extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import certifi\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull articles with sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "\n",
    "def get_jsonparsed_data(url):\n",
    "    \"\"\"\n",
    "    Fetch JSON data from the given URL.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL to fetch data from.\n",
    "\n",
    "    Returns:\n",
    "    dict: The parsed JSON data.\n",
    "    \"\"\"\n",
    "    response = urlopen(url, cafile=certifi.where())\n",
    "    data = response.read().decode(\"utf-8\")\n",
    "    return json.loads(data)\n",
    "\n",
    "def fetch_stock_news_sentiments(api_key, page=0):\n",
    "    \"\"\"\n",
    "    Fetch stock news sentiments from Financial Modeling Prep API.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    page (int): Page number for pagination. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "    list: The parsed JSON data containing news sentiments.\n",
    "    \"\"\"\n",
    "    url = f\"https://financialmodelingprep.com/api/v4/stock-news-sentiments-rss-feed?page={page}&apikey={api_key}\"\n",
    "    return get_jsonparsed_data(url)\n",
    "\n",
    "def save_news_sentiments_to_file(all_news_sentiments, filename):\n",
    "    \"\"\"\n",
    "    Save all news sentiments to a single JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    all_news_sentiments (list): The list of all news sentiments to save.\n",
    "    filename (str): The name of the file to save the news sentiments.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(all_news_sentiments, file, indent=4)\n",
    "    print(f\"All news sentiments saved to {filename}\")\n",
    "\n",
    "def fetch_and_save_all_news(api_key, max_pages=5):\n",
    "    \"\"\"\n",
    "    Fetch and save news articles into a single JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    max_pages (int): Maximum number of pages to fetch. Default is 5.\n",
    "    \"\"\"\n",
    "    all_news_sentiments = []\n",
    "    for page in range(max_pages):\n",
    "        print(f\"Fetching page {page}...\")\n",
    "        news_sentiments = fetch_stock_news_sentiments(api_key, page)\n",
    "        if not news_sentiments:\n",
    "            break  # Exit the loop if no more news is returned\n",
    "        all_news_sentiments.extend(news_sentiments)\n",
    "        time.sleep(.1)  # Sleep to avoid hitting API rate limits\n",
    "\n",
    "    save_news_sentiments_to_file(all_news_sentiments, 'all_news_sentiments.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "api_key = '28a3eef526c43ab5888ab02222aada18'\n",
    "\n",
    "# Specify the number of pages you want to fetch\n",
    "max_pages = 500\n",
    "\n",
    "fetch_and_save_all_news(api_key, max_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def json_to_dataframe(json_file):\n",
    "    \"\"\"\n",
    "    Convert JSON file to a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    json_file (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame containing the JSON data.\n",
    "    \"\"\"\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Create a DataFrame from the list of articles\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "json_file = 'test.json'\n",
    "df = json_to_dataframe(json_file)\n",
    "print(df.head())  # Print the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to CSV and Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_news_sentiments.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('all_news_sentiments.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull Articles with sentiment scores, adjust date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_jsonparsed_data(url):\n",
    "    \"\"\"\n",
    "    Fetch JSON data from the given URL.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL to fetch data from.\n",
    "\n",
    "    Returns:\n",
    "    dict: The parsed JSON data.\n",
    "    \"\"\"\n",
    "    response = urlopen(url, cafile=certifi.where())\n",
    "    data = response.read().decode(\"utf-8\")\n",
    "    return json.loads(data)\n",
    "\n",
    "def fetch_stock_news_sentiments(api_key, ticker, date_from, date_to, limit=50):\n",
    "    \"\"\"\n",
    "    Fetch stock news sentiments from Financial Modeling Prep API.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    ticker (str): The stock ticker symbol (e.g., 'AAPL' for Apple).\n",
    "    date_from (str): Start date for fetching articles (YYYY-MM-DD).\n",
    "    date_to (str): End date for fetching articles (YYYY-MM-DD).\n",
    "    limit (int): Number of articles to fetch per request. Default is 50.\n",
    "\n",
    "    Returns:\n",
    "    list: The parsed JSON data containing news sentiments.\n",
    "    \"\"\"\n",
    "    url = f\"https://financialmodelingprep.com/api/v4/stock-news-sentiments-rss-feed?symbol={ticker}&from={date_from}&to={date_to}&apikey={api_key}&limit={limit}\"\n",
    "    return get_jsonparsed_data(url)\n",
    "\n",
    "def save_news_sentiments_to_file(all_news_sentiments, filename):\n",
    "    \"\"\"\n",
    "    Save all news sentiments to a single JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    all_news_sentiments (list): The list of all news sentiments to save.\n",
    "    filename (str): The name of the file to save the news sentiments.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(all_news_sentiments, file, indent=4)\n",
    "    print(f\"All news sentiments saved to {filename}\")\n",
    "\n",
    "def fetch_and_save_all_news(api_key, tickers, start_date, end_date, interval_days=7, limit=10):\n",
    "    \"\"\"\n",
    "    Fetch and save news articles for a list of tickers into a single JSON file over a specified timeframe.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    tickers (list): A list of ticker symbols.\n",
    "    start_date (str): Start date for fetching articles (YYYY-MM-DD).\n",
    "    end_date (str): End date for fetching articles (YYYY-MM-DD).\n",
    "    interval_days (int): Number of days in each interval for fetching articles. Default is 7.\n",
    "    limit (int): Number of articles to fetch per request. Default is 50.\n",
    "    \"\"\"\n",
    "    all_news_sentiments = []\n",
    "    current_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    while current_date < end_date:\n",
    "        next_date = current_date + timedelta(days=interval_days)\n",
    "        for ticker in tickers:\n",
    "            print(f\"Fetching news for {ticker} from {current_date.strftime('%Y-%m-%d')} to {next_date.strftime('%Y-%m-%d')}...\")\n",
    "            news_sentiments = fetch_stock_news_sentiments(api_key, ticker, current_date.strftime('%Y-%m-%d'), next_date.strftime('%Y-%m-%d'), limit)\n",
    "            if news_sentiments:\n",
    "                all_news_sentiments.extend(news_sentiments)\n",
    "            time.sleep(1)  # Sleep to avoid hitting API rate limits\n",
    "        current_date = next_date\n",
    "\n",
    "    save_news_sentiments_to_file(all_news_sentiments, f'{savename}.json')\n",
    "\n",
    "def fetch_sp500_tickers():\n",
    "    \"\"\"\n",
    "    Fetch the list of S&P 500 ticker symbols from Wikipedia.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of S&P 500 ticker symbols.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data from Wikipedia: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', {'id': 'constituents'})\n",
    "    tickers = []\n",
    "\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        ticker = row.find_all('td')[0].text.strip()\n",
    "        tickers.append(ticker)\n",
    "\n",
    "    return tickers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "api_key = '28a3eef526c43ab5888ab02222aada18'\n",
    "sp500_tickers = fetch_sp500_tickers()\n",
    "sp500_tickers = sp500_tickers[:1]  # Fetch news for the first n tickers\n",
    "savename = 'test'\n",
    "\n",
    "# Specify the date range\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-02-28'\n",
    "\n",
    "if sp500_tickers:\n",
    "    fetch_and_save_all_news(api_key, ['AAPL'], start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def json_to_dataframe(json_file):\n",
    "    \"\"\"\n",
    "    Convert JSON file to a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    json_file (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame containing the JSON data.\n",
    "    \"\"\"\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Create a DataFrame from the list of articles\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "json_file = 'test.json'\n",
    "df = json_to_dataframe(json_file)\n",
    "print(df.head())  # Print the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to CSV and Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{savename}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('all_news_sentiments.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Stock news Extractor\n",
    "For extracting general stock news. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_stock_news(api_key, ticker, date_from, date_to, limit=50):\n",
    "    \"\"\"\n",
    "    Fetch stock news from Financial Modeling Prep API.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    ticker (str): The stock ticker symbol (e.g., 'AAPL' for Apple).\n",
    "    date_from (str): Start date for fetching articles (YYYY-MM-DD).\n",
    "    date_to (str): End date for fetching articles (YYYY-MM-DD).\n",
    "    limit (int): Number of articles to fetch per request. Default is 50.\n",
    "\n",
    "    Returns:\n",
    "    list: The parsed JSON data containing news articles.\n",
    "    \"\"\"\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/stock_news?tickers={ticker}&from={date_from}&to={date_to}&limit={limit}&apikey={api_key}\"\n",
    "    return get_jsonparsed_data(url)\n",
    "\n",
    "def save_news_to_file(all_news, filename):\n",
    "    \"\"\"\n",
    "    Save all news articles to a single JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    all_news (list): The list of all news articles to save.\n",
    "    filename (str): The name of the file to save the news articles.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(all_news, file, indent=4)\n",
    "    print(f\"All news articles saved to {filename}\")\n",
    "\n",
    "def fetch_and_save_all_news(api_key, tickers, start_date, end_date, interval_days=7, max_articles_per_interval=50):\n",
    "    \"\"\"\n",
    "    Fetch and save news articles for a list of tickers into a single JSON file over a specified timeframe.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    tickers (list): A list of ticker symbols.\n",
    "    start_date (str): Start date for fetching articles (YYYY-MM-DD).\n",
    "    end_date (str): End date for fetching articles (YYYY-MM-DD).\n",
    "    interval_days (int): Number of days in each interval for fetching articles. Default is 7.\n",
    "    max_articles_per_interval (int): Maximum number of articles to fetch per interval. Default is 50.\n",
    "    \"\"\"\n",
    "    all_news = []\n",
    "    current_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    while current_date < end_date:\n",
    "        next_date = current_date + timedelta(days=interval_days)\n",
    "        for ticker in tickers:\n",
    "            print(f\"Fetching news for {ticker} from {current_date.strftime('%Y-%m-%d')} to {next_date.strftime('%Y-%m-%d')}...\")\n",
    "            random_start_date = current_date + timedelta(days=random.randint(0, interval_days - 1))\n",
    "            random_end_date = random_start_date + timedelta(days=random.randint(1, interval_days - (random_start_date - current_date).days))\n",
    "            limit = random.randint(1, max_articles_per_interval)\n",
    "            news_articles = fetch_stock_news(api_key, ticker, random_start_date.strftime('%Y-%m-%d'), random_end_date.strftime('%Y-%m-%d'), limit)\n",
    "            if news_articles:\n",
    "                # Filter articles to include only those for the specified ticker\n",
    "                filtered_news_articles = [article for article in news_articles if ticker in article.get('symbol', '')]\n",
    "                all_news.extend(filtered_news_articles)\n",
    "            time.sleep(1)  # Sleep to avoid hitting API rate limits\n",
    "        current_date = next_date\n",
    "\n",
    "    save_news_to_file(all_news, 'all_stock_news.json')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "api_key = '28a3eef526c43ab5888ab02222aada18'\n",
    "sp500_tickers = fetch_sp500_tickers()\n",
    "\n",
    "# Limit the tickers for testing\n",
    "limited_tickers = sp500_tickers #[:1]\n",
    "\n",
    "# Specify the date range\n",
    "start_date = '2023-10-01'\n",
    "end_date = '2023-12-31'\n",
    "\n",
    "if limited_tickers:\n",
    "    fetch_and_save_all_news(api_key, limited_tickers, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def json_to_dataframe(json_file):\n",
    "    \"\"\"\n",
    "    Convert JSON file to a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    json_file (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame containing the JSON data.\n",
    "    \"\"\"\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Create a DataFrame from the list of articles\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "json_file = 'all_stock_news.json'\n",
    "df = json_to_dataframe(json_file)\n",
    "print(df.head())  # Print the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to CSV and Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_news.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('all_news.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull Stock Price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_historical_stock_price(api_key, ticker, date_from, date_to):\n",
    "    \"\"\"\n",
    "    Fetch historical stock price data from Financial Modeling Prep API.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    ticker (str): The stock ticker symbol (e.g., 'AAPL' for Apple).\n",
    "    date_from (str): Start date for fetching data (YYYY-MM-DD).\n",
    "    date_to (str): End date for fetching data (YYYY-MM-DD).\n",
    "\n",
    "    Returns:\n",
    "    list: The parsed JSON data containing historical stock prices.\n",
    "    \"\"\"\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/historical-price-full/{ticker}?from={date_from}&to={date_to}&apikey={api_key}\"\n",
    "    return get_jsonparsed_data(url)\n",
    "\n",
    "def save_historical_prices_to_file(all_prices, filename):\n",
    "    \"\"\"\n",
    "    Save all historical stock prices to a single JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    all_prices (list): The list of all historical stock prices to save.\n",
    "    filename (str): The name of the file to save the historical stock prices.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(all_prices, file, indent=4)\n",
    "    print(f\"All historical stock prices saved to {filename}\")\n",
    "\n",
    "def fetch_and_save_all_historical_prices(api_key, tickers, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch and save historical stock prices for a list of tickers into a single JSON file over a specified timeframe.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    tickers (list): A list of ticker symbols.\n",
    "    start_date (str): Start date for fetching data (YYYY-MM-DD).\n",
    "    end_date (str): End date for fetching data (YYYY-MM-DD).\n",
    "    \"\"\"\n",
    "    all_historical_prices = []\n",
    "    for ticker in tickers:\n",
    "        print(f\"Fetching historical prices for {ticker} from {start_date} to {end_date}...\")\n",
    "        historical_prices = fetch_historical_stock_price(api_key, ticker, start_date, end_date)\n",
    "        if historical_prices:\n",
    "            all_historical_prices.append({ticker: historical_prices})\n",
    "        time.sleep(1)  # Sleep to avoid hitting API rate limits\n",
    "\n",
    "    save_historical_prices_to_file(all_historical_prices, 'all_historical_prices.json')\n",
    "\n",
    "def fetch_sp500_tickers():\n",
    "    \"\"\"\n",
    "    Fetch the list of S&P 500 ticker symbols from Wikipedia.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of S&P 500 ticker symbols.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data from Wikipedia: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', {'id': 'constituents'})\n",
    "    tickers = []\n",
    "\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        ticker = row.find_all('td')[0].text.strip()\n",
    "        tickers.append(ticker)\n",
    "\n",
    "    return tickers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "api_key = '28a3eef526c43ab5888ab02222aada18'\n",
    "sp500_tickers = fetch_sp500_tickers()\n",
    "\n",
    "# Limit to the first ticker (MMM)\n",
    "limited_tickers = sp500_tickers #[:1]\n",
    "\n",
    "# Specify the date range\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-12-31'\n",
    "\n",
    "if limited_tickers:\n",
    "    fetch_and_save_all_historical_prices(api_key, limited_tickers, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert prices json data to pandas dataframe, csv, parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def json_to_dataframe(json_file):\n",
    "    \"\"\"\n",
    "    Convert JSON file to a pandas DataFrame in tidy data format.\n",
    "\n",
    "    Parameters:\n",
    "    json_file (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame in tidy data format.\n",
    "    \"\"\"\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    records = []\n",
    "    for ticker_data in data:\n",
    "        for ticker, content in ticker_data.items():\n",
    "            historical_data = content[\"historical\"]\n",
    "            for price in historical_data:\n",
    "                price['ticker'] = ticker\n",
    "                records.append(price)\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "def save_dataframe_to_csv(df, csv_file):\n",
    "    \"\"\"\n",
    "    Save DataFrame to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The pandas DataFrame to save.\n",
    "    csv_file (str): The name of the CSV file.\n",
    "    \"\"\"\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"Data saved to {csv_file}\")\n",
    "\n",
    "def save_dataframe_to_parquet(df, parquet_file):\n",
    "    \"\"\"\n",
    "    Save DataFrame to a Parquet file.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The pandas DataFrame to save.\n",
    "    parquet_file (str): The name of the Parquet file.\n",
    "    \"\"\"\n",
    "    df.to_parquet(parquet_file, index=False)\n",
    "    print(f\"Data saved to {parquet_file}\")\n",
    "\n",
    "# Example usage\n",
    "json_file = 'all_historical_prices.json'\n",
    "csv_file = 'all_historical_prices.csv'\n",
    "parquet_file = 'all_historical_prices.parquet'\n",
    "\n",
    "# Convert JSON to DataFrame\n",
    "df = json_to_dataframe(json_file)\n",
    "\n",
    "# Check if all columns are retained\n",
    "print(\"Columns in DataFrame:\", df.columns)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "save_dataframe_to_csv(df, csv_file)\n",
    "\n",
    "# Save DataFrame to Parquet\n",
    "save_dataframe_to_parquet(df, parquet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py31",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
