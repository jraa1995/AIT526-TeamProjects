{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Data Extractor\n",
    "Last code chunk is what worked. I'll clean up this notebook this week and refine the data extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import certifi\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull articles with sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_jsonparsed_data(url):\n",
    "    \"\"\"\n",
    "    Fetch JSON data from the given URL.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL to fetch data from.\n",
    "\n",
    "    Returns:\n",
    "    dict: The parsed JSON data.\n",
    "    \"\"\"\n",
    "    response = urlopen(url, cafile=certifi.where())\n",
    "    data = response.read().decode(\"utf-8\")\n",
    "    return json.loads(data)\n",
    "\n",
    "def fetch_stock_news_sentiments(api_key, ticker, page=0):\n",
    "    \"\"\"\n",
    "    Fetch stock news sentiments from Financial Modeling Prep API.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    ticker (str): The stock ticker symbol (e.g., 'AAPL' for Apple).\n",
    "    page (int): Page number for pagination. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "    dict: The parsed JSON data containing news sentiments.\n",
    "    \"\"\"\n",
    "    url = f\"https://financialmodelingprep.com/api/v4/stock-news-sentiments-rss-feed?symbol={ticker}&page={page}&apikey={api_key}\"\n",
    "    return get_jsonparsed_data(url)\n",
    "\n",
    "def save_news_sentiments_to_file(all_news_sentiments, filename):\n",
    "    \"\"\"\n",
    "    Save all news sentiments to a single JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    all_news_sentiments (list): The list of all news sentiments to save.\n",
    "    filename (str): The name of the file to save the news sentiments.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(all_news_sentiments, file, indent=4)\n",
    "    print(f\"All news sentiments saved to {filename}\")\n",
    "\n",
    "def fetch_and_save_all_news(api_key, tickers, page=0):\n",
    "    \"\"\"\n",
    "    Fetch and save news articles for a list of tickers into a single JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    tickers (list): A list of ticker symbols.\n",
    "    page (int): Page number for pagination. Default is 0.\n",
    "    \"\"\"\n",
    "    all_news_sentiments = []\n",
    "    for ticker in tickers:\n",
    "        print(f\"Fetching news for {ticker}...\")\n",
    "        news_sentiments = fetch_stock_news_sentiments(api_key, ticker, page)\n",
    "        if news_sentiments:\n",
    "            all_news_sentiments.extend(news_sentiments)\n",
    "        time.sleep(1)  # Sleep to avoid hitting API rate limits\n",
    "\n",
    "    save_news_sentiments_to_file(all_news_sentiments, 'all_news_sentiments.json')\n",
    "\n",
    "def fetch_sp500_tickers():\n",
    "    \"\"\"\n",
    "    Fetch the list of S&P 500 ticker symbols from Wikipedia.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of S&P 500 ticker symbols.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data from Wikipedia: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', {'id': 'constituents'})\n",
    "    tickers = []\n",
    "\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        ticker = row.find_all('td')[0].text.strip()\n",
    "        tickers.append(ticker)\n",
    "\n",
    "    return tickers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "api_key = '28a3eef526c43ab5888ab02222aada18'\n",
    "# fetch top # tickers\n",
    "#sp500_tickers = sp500_tickers[:5]  \n",
    "sp500_tickers = fetch_sp500_tickers()\n",
    "\n",
    "if sp500_tickers:\n",
    "    fetch_and_save_all_news(api_key, sp500_tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  symbol             publishedDate  \\\n",
      "0    NOK  2024-06-28T16:16:07.000Z   \n",
      "1   MASI  2024-06-28T16:16:04.000Z   \n",
      "2   AAPX  2024-06-28T16:16:01.000Z   \n",
      "3    MFC  2024-06-28T16:15:59.000Z   \n",
      "4   PATH  2024-06-28T16:15:00.000Z   \n",
      "\n",
      "                                               title  \\\n",
      "0  Nokia (NOK) Set to Reboot Infrastructure Busin...   \n",
      "1  Masimo (MASI), Cleveland Clinic Unite to Enhan...   \n",
      "2  5 Leveraged ETFs That Have Gained in Double Di...   \n",
      "3  Manulife Financial (MFC) Shares Gain on New Fi...   \n",
      "4  Edelson Lechtzin LLP Urges UiPath, Inc. (NYSE:...   \n",
      "\n",
      "                                               image            site  \\\n",
      "0  https://cdn.snapi.dev/images/v1/i/a/communicat...       zacks.com   \n",
      "1  https://cdn.snapi.dev/images/v1/u/9/medical25-...       zacks.com   \n",
      "2  https://cdn.snapi.dev/images/v1/t/n/etf30-2502...       zacks.com   \n",
      "3  https://cdn.snapi.dev/images/v1/b/l/insurance9...       zacks.com   \n",
      "4  https://cdn.snapi.dev/images/v1/g/w/press15-25...  accesswire.com   \n",
      "\n",
      "                                                text  \\\n",
      "0  Nokia (NOK) is set to acquire Infinera for an ...   \n",
      "1  Masimo (MASI) collaborates with Cleveland Clin...   \n",
      "2  Wall Street has been on a solid rally in June,...   \n",
      "3  Manulife Financial (MFC) sets new financial go...   \n",
      "4  Edelson Lechtzin LLP is investigating securiti...   \n",
      "\n",
      "                                                 url sentiment  sentimentScore  \n",
      "0  https://www.zacks.com/stock/news/2295087/nokia...  Positive           0.400  \n",
      "1  https://www.zacks.com/stock/news/2295086/masim...  Positive           0.300  \n",
      "2  https://www.zacks.com/stock/news/2295088/5-lev...  Positive           0.400  \n",
      "3  https://www.zacks.com/stock/news/2295089/manul...   Neutral           0.000  \n",
      "4  https://www.accesswire.com/viewarticle.aspx?id...  Negative          -0.366  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def json_to_dataframe(json_file):\n",
    "    \"\"\"\n",
    "    Convert JSON file to a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    json_file (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame containing the JSON data.\n",
    "    \"\"\"\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Create a DataFrame from the list of articles\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "json_file = 'test.json'\n",
    "df = json_to_dataframe(json_file)\n",
    "print(df.head())  # Print the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to CSV and Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{savename}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('all_news_sentiments.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull Articles with sentiment scores, adjust date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_jsonparsed_data(url):\n",
    "    \"\"\"\n",
    "    Fetch JSON data from the given URL.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL to fetch data from.\n",
    "\n",
    "    Returns:\n",
    "    dict: The parsed JSON data.\n",
    "    \"\"\"\n",
    "    response = urlopen(url, cafile=certifi.where())\n",
    "    data = response.read().decode(\"utf-8\")\n",
    "    return json.loads(data)\n",
    "\n",
    "def fetch_stock_news_sentiments(api_key, ticker, date_from, date_to, limit=50):\n",
    "    \"\"\"\n",
    "    Fetch stock news sentiments from Financial Modeling Prep API.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    ticker (str): The stock ticker symbol (e.g., 'AAPL' for Apple).\n",
    "    date_from (str): Start date for fetching articles (YYYY-MM-DD).\n",
    "    date_to (str): End date for fetching articles (YYYY-MM-DD).\n",
    "    limit (int): Number of articles to fetch per request. Default is 50.\n",
    "\n",
    "    Returns:\n",
    "    list: The parsed JSON data containing news sentiments.\n",
    "    \"\"\"\n",
    "    url = f\"https://financialmodelingprep.com/api/v4/stock-news-sentiments-rss-feed?symbol={ticker}&from={date_from}&to={date_to}&apikey={api_key}&limit={limit}\"\n",
    "    return get_jsonparsed_data(url)\n",
    "\n",
    "def save_news_sentiments_to_file(all_news_sentiments, filename):\n",
    "    \"\"\"\n",
    "    Save all news sentiments to a single JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    all_news_sentiments (list): The list of all news sentiments to save.\n",
    "    filename (str): The name of the file to save the news sentiments.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(all_news_sentiments, file, indent=4)\n",
    "    print(f\"All news sentiments saved to {filename}\")\n",
    "\n",
    "def fetch_and_save_all_news(api_key, tickers, start_date, end_date, interval_days=7, limit=10):\n",
    "    \"\"\"\n",
    "    Fetch and save news articles for a list of tickers into a single JSON file over a specified timeframe.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    tickers (list): A list of ticker symbols.\n",
    "    start_date (str): Start date for fetching articles (YYYY-MM-DD).\n",
    "    end_date (str): End date for fetching articles (YYYY-MM-DD).\n",
    "    interval_days (int): Number of days in each interval for fetching articles. Default is 7.\n",
    "    limit (int): Number of articles to fetch per request. Default is 50.\n",
    "    \"\"\"\n",
    "    all_news_sentiments = []\n",
    "    current_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    while current_date < end_date:\n",
    "        next_date = current_date + timedelta(days=interval_days)\n",
    "        for ticker in tickers:\n",
    "            print(f\"Fetching news for {ticker} from {current_date.strftime('%Y-%m-%d')} to {next_date.strftime('%Y-%m-%d')}...\")\n",
    "            news_sentiments = fetch_stock_news_sentiments(api_key, ticker, current_date.strftime('%Y-%m-%d'), next_date.strftime('%Y-%m-%d'), limit)\n",
    "            if news_sentiments:\n",
    "                all_news_sentiments.extend(news_sentiments)\n",
    "            time.sleep(1)  # Sleep to avoid hitting API rate limits\n",
    "        current_date = next_date\n",
    "\n",
    "    save_news_sentiments_to_file(all_news_sentiments, f'{savename}.json')\n",
    "\n",
    "def fetch_sp500_tickers():\n",
    "    \"\"\"\n",
    "    Fetch the list of S&P 500 ticker symbols from Wikipedia.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of S&P 500 ticker symbols.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data from Wikipedia: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', {'id': 'constituents'})\n",
    "    tickers = []\n",
    "\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        ticker = row.find_all('td')[0].text.strip()\n",
    "        tickers.append(ticker)\n",
    "\n",
    "    return tickers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news for AAPL from 2023-01-01 to 2023-01-08...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j2/skjc081x2h5cd717r89c0q480000gn/T/ipykernel_80729/34511153.py:19: DeprecationWarning: cafile, capath and cadefault are deprecated, use a custom context instead.\n",
      "  response = urlopen(url, cafile=certifi.where())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news for AAPL from 2023-01-08 to 2023-01-15...\n",
      "Fetching news for AAPL from 2023-01-15 to 2023-01-22...\n",
      "Fetching news for AAPL from 2023-01-22 to 2023-01-29...\n",
      "Fetching news for AAPL from 2023-01-29 to 2023-02-05...\n",
      "Fetching news for AAPL from 2023-02-05 to 2023-02-12...\n",
      "Fetching news for AAPL from 2023-02-12 to 2023-02-19...\n",
      "Fetching news for AAPL from 2023-02-19 to 2023-02-26...\n",
      "Fetching news for AAPL from 2023-02-26 to 2023-03-05...\n",
      "All news sentiments saved to test.json\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "api_key = '28a3eef526c43ab5888ab02222aada18'\n",
    "sp500_tickers = fetch_sp500_tickers()\n",
    "sp500_tickers = sp500_tickers[:1]  # Fetch news for the first n tickers\n",
    "savename = 'test'\n",
    "\n",
    "# Specify the date range\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-02-28'\n",
    "\n",
    "if sp500_tickers:\n",
    "    fetch_and_save_all_news(api_key, ['AAPL'], start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  symbol             publishedDate  \\\n",
      "0    NOK  2024-06-28T16:16:07.000Z   \n",
      "1   MASI  2024-06-28T16:16:04.000Z   \n",
      "2   AAPX  2024-06-28T16:16:01.000Z   \n",
      "3    MFC  2024-06-28T16:15:59.000Z   \n",
      "4   PATH  2024-06-28T16:15:00.000Z   \n",
      "\n",
      "                                               title  \\\n",
      "0  Nokia (NOK) Set to Reboot Infrastructure Busin...   \n",
      "1  Masimo (MASI), Cleveland Clinic Unite to Enhan...   \n",
      "2  5 Leveraged ETFs That Have Gained in Double Di...   \n",
      "3  Manulife Financial (MFC) Shares Gain on New Fi...   \n",
      "4  Edelson Lechtzin LLP Urges UiPath, Inc. (NYSE:...   \n",
      "\n",
      "                                               image            site  \\\n",
      "0  https://cdn.snapi.dev/images/v1/i/a/communicat...       zacks.com   \n",
      "1  https://cdn.snapi.dev/images/v1/u/9/medical25-...       zacks.com   \n",
      "2  https://cdn.snapi.dev/images/v1/t/n/etf30-2502...       zacks.com   \n",
      "3  https://cdn.snapi.dev/images/v1/b/l/insurance9...       zacks.com   \n",
      "4  https://cdn.snapi.dev/images/v1/g/w/press15-25...  accesswire.com   \n",
      "\n",
      "                                                text  \\\n",
      "0  Nokia (NOK) is set to acquire Infinera for an ...   \n",
      "1  Masimo (MASI) collaborates with Cleveland Clin...   \n",
      "2  Wall Street has been on a solid rally in June,...   \n",
      "3  Manulife Financial (MFC) sets new financial go...   \n",
      "4  Edelson Lechtzin LLP is investigating securiti...   \n",
      "\n",
      "                                                 url sentiment  sentimentScore  \n",
      "0  https://www.zacks.com/stock/news/2295087/nokia...  Positive           0.400  \n",
      "1  https://www.zacks.com/stock/news/2295086/masim...  Positive           0.300  \n",
      "2  https://www.zacks.com/stock/news/2295088/5-lev...  Positive           0.400  \n",
      "3  https://www.zacks.com/stock/news/2295089/manul...   Neutral           0.000  \n",
      "4  https://www.accesswire.com/viewarticle.aspx?id...  Negative          -0.366  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def json_to_dataframe(json_file):\n",
    "    \"\"\"\n",
    "    Convert JSON file to a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    json_file (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame containing the JSON data.\n",
    "    \"\"\"\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Create a DataFrame from the list of articles\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "json_file = 'test.json'\n",
    "df = json_to_dataframe(json_file)\n",
    "print(df.head())  # Print the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to CSV and Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{savename}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('all_news_sentiments.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Stock news Extractor\n",
    "For extracting general stock news. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_stock_news(api_key, ticker, date_from, date_to, limit=50):\n",
    "    \"\"\"\n",
    "    Fetch stock news from Financial Modeling Prep API.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    ticker (str): The stock ticker symbol (e.g., 'AAPL' for Apple).\n",
    "    date_from (str): Start date for fetching articles (YYYY-MM-DD).\n",
    "    date_to (str): End date for fetching articles (YYYY-MM-DD).\n",
    "    limit (int): Number of articles to fetch per request. Default is 50.\n",
    "\n",
    "    Returns:\n",
    "    list: The parsed JSON data containing news articles.\n",
    "    \"\"\"\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/stock_news?tickers={ticker}&from={date_from}&to={date_to}&limit={limit}&apikey={api_key}\"\n",
    "    return get_jsonparsed_data(url)\n",
    "\n",
    "def save_news_to_file(all_news, filename):\n",
    "    \"\"\"\n",
    "    Save all news articles to a single JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    all_news (list): The list of all news articles to save.\n",
    "    filename (str): The name of the file to save the news articles.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(all_news, file, indent=4)\n",
    "    print(f\"All news articles saved to {filename}\")\n",
    "\n",
    "def fetch_and_save_all_news(api_key, tickers, start_date, end_date, interval_days=7, max_articles_per_interval=50):\n",
    "    \"\"\"\n",
    "    Fetch and save news articles for a list of tickers into a single JSON file over a specified timeframe.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    tickers (list): A list of ticker symbols.\n",
    "    start_date (str): Start date for fetching articles (YYYY-MM-DD).\n",
    "    end_date (str): End date for fetching articles (YYYY-MM-DD).\n",
    "    interval_days (int): Number of days in each interval for fetching articles. Default is 7.\n",
    "    max_articles_per_interval (int): Maximum number of articles to fetch per interval. Default is 50.\n",
    "    \"\"\"\n",
    "    all_news = []\n",
    "    current_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    while current_date < end_date:\n",
    "        next_date = current_date + timedelta(days=interval_days)\n",
    "        for ticker in tickers:\n",
    "            print(f\"Fetching news for {ticker} from {current_date.strftime('%Y-%m-%d')} to {next_date.strftime('%Y-%m-%d')}...\")\n",
    "            random_start_date = current_date + timedelta(days=random.randint(0, interval_days - 1))\n",
    "            random_end_date = random_start_date + timedelta(days=random.randint(1, interval_days - (random_start_date - current_date).days))\n",
    "            limit = random.randint(1, max_articles_per_interval)\n",
    "            news_articles = fetch_stock_news(api_key, ticker, random_start_date.strftime('%Y-%m-%d'), random_end_date.strftime('%Y-%m-%d'), limit)\n",
    "            if news_articles:\n",
    "                # Filter articles to include only those for the specified ticker\n",
    "                filtered_news_articles = [article for article in news_articles if ticker in article.get('symbol', '')]\n",
    "                all_news.extend(filtered_news_articles)\n",
    "            time.sleep(1)  # Sleep to avoid hitting API rate limits\n",
    "        current_date = next_date\n",
    "\n",
    "    save_news_to_file(all_news, 'all_stock_news.json')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news for MMM from 2023-01-01 to 2023-01-08...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j2/skjc081x2h5cd717r89c0q480000gn/T/ipykernel_80729/34511153.py:19: DeprecationWarning: cafile, capath and cadefault are deprecated, use a custom context instead.\n",
      "  response = urlopen(url, cafile=certifi.where())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news for MMM from 2023-01-08 to 2023-01-15...\n",
      "Fetching news for MMM from 2023-01-15 to 2023-01-22...\n",
      "Fetching news for MMM from 2023-01-22 to 2023-01-29...\n",
      "Fetching news for MMM from 2023-01-29 to 2023-02-05...\n",
      "All news articles saved to all_stock_news.json\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "api_key = '28a3eef526c43ab5888ab02222aada18'\n",
    "sp500_tickers = fetch_sp500_tickers()\n",
    "\n",
    "# Limit the tickers for testing\n",
    "limited_tickers = sp500_tickers[:1]\n",
    "\n",
    "# Specify the date range\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-02-05'\n",
    "\n",
    "if limited_tickers:\n",
    "    fetch_and_save_all_news(api_key, limited_tickers, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  symbol             publishedDate  \\\n",
      "0    NOK  2024-06-28T16:16:07.000Z   \n",
      "1   MASI  2024-06-28T16:16:04.000Z   \n",
      "2   AAPX  2024-06-28T16:16:01.000Z   \n",
      "3    MFC  2024-06-28T16:15:59.000Z   \n",
      "4   PATH  2024-06-28T16:15:00.000Z   \n",
      "\n",
      "                                               title  \\\n",
      "0  Nokia (NOK) Set to Reboot Infrastructure Busin...   \n",
      "1  Masimo (MASI), Cleveland Clinic Unite to Enhan...   \n",
      "2  5 Leveraged ETFs That Have Gained in Double Di...   \n",
      "3  Manulife Financial (MFC) Shares Gain on New Fi...   \n",
      "4  Edelson Lechtzin LLP Urges UiPath, Inc. (NYSE:...   \n",
      "\n",
      "                                               image            site  \\\n",
      "0  https://cdn.snapi.dev/images/v1/i/a/communicat...       zacks.com   \n",
      "1  https://cdn.snapi.dev/images/v1/u/9/medical25-...       zacks.com   \n",
      "2  https://cdn.snapi.dev/images/v1/t/n/etf30-2502...       zacks.com   \n",
      "3  https://cdn.snapi.dev/images/v1/b/l/insurance9...       zacks.com   \n",
      "4  https://cdn.snapi.dev/images/v1/g/w/press15-25...  accesswire.com   \n",
      "\n",
      "                                                text  \\\n",
      "0  Nokia (NOK) is set to acquire Infinera for an ...   \n",
      "1  Masimo (MASI) collaborates with Cleveland Clin...   \n",
      "2  Wall Street has been on a solid rally in June,...   \n",
      "3  Manulife Financial (MFC) sets new financial go...   \n",
      "4  Edelson Lechtzin LLP is investigating securiti...   \n",
      "\n",
      "                                                 url sentiment  sentimentScore  \n",
      "0  https://www.zacks.com/stock/news/2295087/nokia...  Positive           0.400  \n",
      "1  https://www.zacks.com/stock/news/2295086/masim...  Positive           0.300  \n",
      "2  https://www.zacks.com/stock/news/2295088/5-lev...  Positive           0.400  \n",
      "3  https://www.zacks.com/stock/news/2295089/manul...   Neutral           0.000  \n",
      "4  https://www.accesswire.com/viewarticle.aspx?id...  Negative          -0.366  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def json_to_dataframe(json_file):\n",
    "    \"\"\"\n",
    "    Convert JSON file to a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    json_file (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame containing the JSON data.\n",
    "    \"\"\"\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Create a DataFrame from the list of articles\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "json_file = 'test.json'\n",
    "df = json_to_dataframe(json_file)\n",
    "print(df.head())  # Print the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to CSV and Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{savename}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('all_news_sentiments.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py31",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
