{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Data Extractor\n",
    "Last code chunk is what worked. I'll clean up this notebook this week and refine the data extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import certifi\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull articles with sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_jsonparsed_data(url):\n",
    "    \"\"\"\n",
    "    Fetch JSON data from the given URL.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL to fetch data from.\n",
    "\n",
    "    Returns:\n",
    "    dict: The parsed JSON data.\n",
    "    \"\"\"\n",
    "    response = urlopen(url, cafile=certifi.where())\n",
    "    data = response.read().decode(\"utf-8\")\n",
    "    return json.loads(data)\n",
    "\n",
    "def fetch_stock_news_sentiments(api_key, ticker, page=0):\n",
    "    \"\"\"\n",
    "    Fetch stock news sentiments from Financial Modeling Prep API.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    ticker (str): The stock ticker symbol (e.g., 'AAPL' for Apple).\n",
    "    page (int): Page number for pagination. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "    dict: The parsed JSON data containing news sentiments.\n",
    "    \"\"\"\n",
    "    url = f\"https://financialmodelingprep.com/api/v4/stock-news-sentiments-rss-feed?symbol={ticker}&page={page}&apikey={api_key}\"\n",
    "    return get_jsonparsed_data(url)\n",
    "\n",
    "def save_news_sentiments_to_file(all_news_sentiments, filename):\n",
    "    \"\"\"\n",
    "    Save all news sentiments to a single JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    all_news_sentiments (list): The list of all news sentiments to save.\n",
    "    filename (str): The name of the file to save the news sentiments.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(all_news_sentiments, file, indent=4)\n",
    "    print(f\"All news sentiments saved to {filename}\")\n",
    "\n",
    "def fetch_and_save_all_news(api_key, tickers, page=0):\n",
    "    \"\"\"\n",
    "    Fetch and save news articles for a list of tickers into a single JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    tickers (list): A list of ticker symbols.\n",
    "    page (int): Page number for pagination. Default is 0.\n",
    "    \"\"\"\n",
    "    all_news_sentiments = []\n",
    "    for ticker in tickers:\n",
    "        print(f\"Fetching news for {ticker}...\")\n",
    "        news_sentiments = fetch_stock_news_sentiments(api_key, ticker, page)\n",
    "        if news_sentiments:\n",
    "            all_news_sentiments.extend(news_sentiments)\n",
    "        time.sleep(1)  # Sleep to avoid hitting API rate limits\n",
    "\n",
    "    save_news_sentiments_to_file(all_news_sentiments, 'all_news_sentiments.json')\n",
    "\n",
    "def fetch_sp500_tickers():\n",
    "    \"\"\"\n",
    "    Fetch the list of S&P 500 ticker symbols from Wikipedia.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of S&P 500 ticker symbols.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data from Wikipedia: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', {'id': 'constituents'})\n",
    "    tickers = []\n",
    "\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        ticker = row.find_all('td')[0].text.strip()\n",
    "        tickers.append(ticker)\n",
    "\n",
    "    return tickers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "api_key = '28a3eef526c43ab5888ab02222aada18'\n",
    "# fetch top # tickers\n",
    "#sp500_tickers = sp500_tickers[:5]  \n",
    "sp500_tickers = fetch_sp500_tickers()\n",
    "\n",
    "if sp500_tickers:\n",
    "    fetch_and_save_all_news(api_key, sp500_tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  symbol             publishedDate  \\\n",
      "0    NOK  2024-06-28T16:16:07.000Z   \n",
      "1   MASI  2024-06-28T16:16:04.000Z   \n",
      "2   AAPX  2024-06-28T16:16:01.000Z   \n",
      "3    MFC  2024-06-28T16:15:59.000Z   \n",
      "4   PATH  2024-06-28T16:15:00.000Z   \n",
      "\n",
      "                                               title  \\\n",
      "0  Nokia (NOK) Set to Reboot Infrastructure Busin...   \n",
      "1  Masimo (MASI), Cleveland Clinic Unite to Enhan...   \n",
      "2  5 Leveraged ETFs That Have Gained in Double Di...   \n",
      "3  Manulife Financial (MFC) Shares Gain on New Fi...   \n",
      "4  Edelson Lechtzin LLP Urges UiPath, Inc. (NYSE:...   \n",
      "\n",
      "                                               image            site  \\\n",
      "0  https://cdn.snapi.dev/images/v1/i/a/communicat...       zacks.com   \n",
      "1  https://cdn.snapi.dev/images/v1/u/9/medical25-...       zacks.com   \n",
      "2  https://cdn.snapi.dev/images/v1/t/n/etf30-2502...       zacks.com   \n",
      "3  https://cdn.snapi.dev/images/v1/b/l/insurance9...       zacks.com   \n",
      "4  https://cdn.snapi.dev/images/v1/g/w/press15-25...  accesswire.com   \n",
      "\n",
      "                                                text  \\\n",
      "0  Nokia (NOK) is set to acquire Infinera for an ...   \n",
      "1  Masimo (MASI) collaborates with Cleveland Clin...   \n",
      "2  Wall Street has been on a solid rally in June,...   \n",
      "3  Manulife Financial (MFC) sets new financial go...   \n",
      "4  Edelson Lechtzin LLP is investigating securiti...   \n",
      "\n",
      "                                                 url sentiment  sentimentScore  \n",
      "0  https://www.zacks.com/stock/news/2295087/nokia...  Positive           0.400  \n",
      "1  https://www.zacks.com/stock/news/2295086/masim...  Positive           0.300  \n",
      "2  https://www.zacks.com/stock/news/2295088/5-lev...  Positive           0.400  \n",
      "3  https://www.zacks.com/stock/news/2295089/manul...   Neutral           0.000  \n",
      "4  https://www.accesswire.com/viewarticle.aspx?id...  Negative          -0.366  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def json_to_dataframe(json_file):\n",
    "    \"\"\"\n",
    "    Convert JSON file to a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    json_file (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame containing the JSON data.\n",
    "    \"\"\"\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Create a DataFrame from the list of articles\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "json_file = 'test.json'\n",
    "df = json_to_dataframe(json_file)\n",
    "print(df.head())  # Print the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to CSV and Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{savename}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('all_news_sentiments.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull Articles with sentiment scores, adjust date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_jsonparsed_data(url):\n",
    "    \"\"\"\n",
    "    Fetch JSON data from the given URL.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL to fetch data from.\n",
    "\n",
    "    Returns:\n",
    "    dict: The parsed JSON data.\n",
    "    \"\"\"\n",
    "    response = urlopen(url, cafile=certifi.where())\n",
    "    data = response.read().decode(\"utf-8\")\n",
    "    return json.loads(data)\n",
    "\n",
    "def fetch_stock_news_sentiments(api_key, ticker, date_from, date_to, limit=50):\n",
    "    \"\"\"\n",
    "    Fetch stock news sentiments from Financial Modeling Prep API.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    ticker (str): The stock ticker symbol (e.g., 'AAPL' for Apple).\n",
    "    date_from (str): Start date for fetching articles (YYYY-MM-DD).\n",
    "    date_to (str): End date for fetching articles (YYYY-MM-DD).\n",
    "    limit (int): Number of articles to fetch per request. Default is 50.\n",
    "\n",
    "    Returns:\n",
    "    list: The parsed JSON data containing news sentiments.\n",
    "    \"\"\"\n",
    "    url = f\"https://financialmodelingprep.com/api/v4/stock-news-sentiments-rss-feed?symbol={ticker}&from={date_from}&to={date_to}&apikey={api_key}&limit={limit}\"\n",
    "    return get_jsonparsed_data(url)\n",
    "\n",
    "def save_news_sentiments_to_file(all_news_sentiments, filename):\n",
    "    \"\"\"\n",
    "    Save all news sentiments to a single JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    all_news_sentiments (list): The list of all news sentiments to save.\n",
    "    filename (str): The name of the file to save the news sentiments.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(all_news_sentiments, file, indent=4)\n",
    "    print(f\"All news sentiments saved to {filename}\")\n",
    "\n",
    "def fetch_and_save_all_news(api_key, tickers, start_date, end_date, interval_days=7, limit=10):\n",
    "    \"\"\"\n",
    "    Fetch and save news articles for a list of tickers into a single JSON file over a specified timeframe.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    tickers (list): A list of ticker symbols.\n",
    "    start_date (str): Start date for fetching articles (YYYY-MM-DD).\n",
    "    end_date (str): End date for fetching articles (YYYY-MM-DD).\n",
    "    interval_days (int): Number of days in each interval for fetching articles. Default is 7.\n",
    "    limit (int): Number of articles to fetch per request. Default is 50.\n",
    "    \"\"\"\n",
    "    all_news_sentiments = []\n",
    "    current_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    while current_date < end_date:\n",
    "        next_date = current_date + timedelta(days=interval_days)\n",
    "        for ticker in tickers:\n",
    "            print(f\"Fetching news for {ticker} from {current_date.strftime('%Y-%m-%d')} to {next_date.strftime('%Y-%m-%d')}...\")\n",
    "            news_sentiments = fetch_stock_news_sentiments(api_key, ticker, current_date.strftime('%Y-%m-%d'), next_date.strftime('%Y-%m-%d'), limit)\n",
    "            if news_sentiments:\n",
    "                all_news_sentiments.extend(news_sentiments)\n",
    "            time.sleep(1)  # Sleep to avoid hitting API rate limits\n",
    "        current_date = next_date\n",
    "\n",
    "    save_news_sentiments_to_file(all_news_sentiments, f'{savename}.json')\n",
    "\n",
    "def fetch_sp500_tickers():\n",
    "    \"\"\"\n",
    "    Fetch the list of S&P 500 ticker symbols from Wikipedia.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of S&P 500 ticker symbols.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data from Wikipedia: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', {'id': 'constituents'})\n",
    "    tickers = []\n",
    "\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        ticker = row.find_all('td')[0].text.strip()\n",
    "        tickers.append(ticker)\n",
    "\n",
    "    return tickers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news for AAPL from 2023-01-01 to 2023-01-08...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j2/skjc081x2h5cd717r89c0q480000gn/T/ipykernel_80729/34511153.py:19: DeprecationWarning: cafile, capath and cadefault are deprecated, use a custom context instead.\n",
      "  response = urlopen(url, cafile=certifi.where())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news for AAPL from 2023-01-08 to 2023-01-15...\n",
      "Fetching news for AAPL from 2023-01-15 to 2023-01-22...\n",
      "Fetching news for AAPL from 2023-01-22 to 2023-01-29...\n",
      "Fetching news for AAPL from 2023-01-29 to 2023-02-05...\n",
      "Fetching news for AAPL from 2023-02-05 to 2023-02-12...\n",
      "Fetching news for AAPL from 2023-02-12 to 2023-02-19...\n",
      "Fetching news for AAPL from 2023-02-19 to 2023-02-26...\n",
      "Fetching news for AAPL from 2023-02-26 to 2023-03-05...\n",
      "All news sentiments saved to test.json\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "api_key = '28a3eef526c43ab5888ab02222aada18'\n",
    "sp500_tickers = fetch_sp500_tickers()\n",
    "sp500_tickers = sp500_tickers[:1]  # Fetch news for the first n tickers\n",
    "savename = 'test'\n",
    "\n",
    "# Specify the date range\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-02-28'\n",
    "\n",
    "if sp500_tickers:\n",
    "    fetch_and_save_all_news(api_key, ['AAPL'], start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  symbol             publishedDate  \\\n",
      "0    NOK  2024-06-28T16:16:07.000Z   \n",
      "1   MASI  2024-06-28T16:16:04.000Z   \n",
      "2   AAPX  2024-06-28T16:16:01.000Z   \n",
      "3    MFC  2024-06-28T16:15:59.000Z   \n",
      "4   PATH  2024-06-28T16:15:00.000Z   \n",
      "\n",
      "                                               title  \\\n",
      "0  Nokia (NOK) Set to Reboot Infrastructure Busin...   \n",
      "1  Masimo (MASI), Cleveland Clinic Unite to Enhan...   \n",
      "2  5 Leveraged ETFs That Have Gained in Double Di...   \n",
      "3  Manulife Financial (MFC) Shares Gain on New Fi...   \n",
      "4  Edelson Lechtzin LLP Urges UiPath, Inc. (NYSE:...   \n",
      "\n",
      "                                               image            site  \\\n",
      "0  https://cdn.snapi.dev/images/v1/i/a/communicat...       zacks.com   \n",
      "1  https://cdn.snapi.dev/images/v1/u/9/medical25-...       zacks.com   \n",
      "2  https://cdn.snapi.dev/images/v1/t/n/etf30-2502...       zacks.com   \n",
      "3  https://cdn.snapi.dev/images/v1/b/l/insurance9...       zacks.com   \n",
      "4  https://cdn.snapi.dev/images/v1/g/w/press15-25...  accesswire.com   \n",
      "\n",
      "                                                text  \\\n",
      "0  Nokia (NOK) is set to acquire Infinera for an ...   \n",
      "1  Masimo (MASI) collaborates with Cleveland Clin...   \n",
      "2  Wall Street has been on a solid rally in June,...   \n",
      "3  Manulife Financial (MFC) sets new financial go...   \n",
      "4  Edelson Lechtzin LLP is investigating securiti...   \n",
      "\n",
      "                                                 url sentiment  sentimentScore  \n",
      "0  https://www.zacks.com/stock/news/2295087/nokia...  Positive           0.400  \n",
      "1  https://www.zacks.com/stock/news/2295086/masim...  Positive           0.300  \n",
      "2  https://www.zacks.com/stock/news/2295088/5-lev...  Positive           0.400  \n",
      "3  https://www.zacks.com/stock/news/2295089/manul...   Neutral           0.000  \n",
      "4  https://www.accesswire.com/viewarticle.aspx?id...  Negative          -0.366  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def json_to_dataframe(json_file):\n",
    "    \"\"\"\n",
    "    Convert JSON file to a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    json_file (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame containing the JSON data.\n",
    "    \"\"\"\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Create a DataFrame from the list of articles\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "json_file = 'test.json'\n",
    "df = json_to_dataframe(json_file)\n",
    "print(df.head())  # Print the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to CSV and Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{savename}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('all_news_sentiments.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Stock news Extractor\n",
    "For extracting general stock news. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_stock_news(api_key, ticker, date_from, date_to, limit=50):\n",
    "    \"\"\"\n",
    "    Fetch stock news from Financial Modeling Prep API.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    ticker (str): The stock ticker symbol (e.g., 'AAPL' for Apple).\n",
    "    date_from (str): Start date for fetching articles (YYYY-MM-DD).\n",
    "    date_to (str): End date for fetching articles (YYYY-MM-DD).\n",
    "    limit (int): Number of articles to fetch per request. Default is 50.\n",
    "\n",
    "    Returns:\n",
    "    list: The parsed JSON data containing news articles.\n",
    "    \"\"\"\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/stock_news?tickers={ticker}&from={date_from}&to={date_to}&limit={limit}&apikey={api_key}\"\n",
    "    return get_jsonparsed_data(url)\n",
    "\n",
    "def save_news_to_file(all_news, filename):\n",
    "    \"\"\"\n",
    "    Save all news articles to a single JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    all_news (list): The list of all news articles to save.\n",
    "    filename (str): The name of the file to save the news articles.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(all_news, file, indent=4)\n",
    "    print(f\"All news articles saved to {filename}\")\n",
    "\n",
    "def fetch_and_save_all_news(api_key, tickers, start_date, end_date, interval_days=7, max_articles_per_interval=50):\n",
    "    \"\"\"\n",
    "    Fetch and save news articles for a list of tickers into a single JSON file over a specified timeframe.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    tickers (list): A list of ticker symbols.\n",
    "    start_date (str): Start date for fetching articles (YYYY-MM-DD).\n",
    "    end_date (str): End date for fetching articles (YYYY-MM-DD).\n",
    "    interval_days (int): Number of days in each interval for fetching articles. Default is 7.\n",
    "    max_articles_per_interval (int): Maximum number of articles to fetch per interval. Default is 50.\n",
    "    \"\"\"\n",
    "    all_news = []\n",
    "    current_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    while current_date < end_date:\n",
    "        next_date = current_date + timedelta(days=interval_days)\n",
    "        for ticker in tickers:\n",
    "            print(f\"Fetching news for {ticker} from {current_date.strftime('%Y-%m-%d')} to {next_date.strftime('%Y-%m-%d')}...\")\n",
    "            random_start_date = current_date + timedelta(days=random.randint(0, interval_days - 1))\n",
    "            random_end_date = random_start_date + timedelta(days=random.randint(1, interval_days - (random_start_date - current_date).days))\n",
    "            limit = random.randint(1, max_articles_per_interval)\n",
    "            news_articles = fetch_stock_news(api_key, ticker, random_start_date.strftime('%Y-%m-%d'), random_end_date.strftime('%Y-%m-%d'), limit)\n",
    "            if news_articles:\n",
    "                # Filter articles to include only those for the specified ticker\n",
    "                filtered_news_articles = [article for article in news_articles if ticker in article.get('symbol', '')]\n",
    "                all_news.extend(filtered_news_articles)\n",
    "            time.sleep(1)  # Sleep to avoid hitting API rate limits\n",
    "        current_date = next_date\n",
    "\n",
    "    save_news_to_file(all_news, 'all_stock_news.json')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news for MMM from 2023-01-01 to 2023-01-08...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j2/skjc081x2h5cd717r89c0q480000gn/T/ipykernel_80729/34511153.py:19: DeprecationWarning: cafile, capath and cadefault are deprecated, use a custom context instead.\n",
      "  response = urlopen(url, cafile=certifi.where())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news for MMM from 2023-01-08 to 2023-01-15...\n",
      "Fetching news for MMM from 2023-01-15 to 2023-01-22...\n",
      "Fetching news for MMM from 2023-01-22 to 2023-01-29...\n",
      "Fetching news for MMM from 2023-01-29 to 2023-02-05...\n",
      "All news articles saved to all_stock_news.json\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "api_key = '28a3eef526c43ab5888ab02222aada18'\n",
    "sp500_tickers = fetch_sp500_tickers()\n",
    "\n",
    "# Limit the tickers for testing\n",
    "limited_tickers = sp500_tickers[:1]\n",
    "\n",
    "# Specify the date range\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-02-05'\n",
    "\n",
    "if limited_tickers:\n",
    "    fetch_and_save_all_news(api_key, limited_tickers, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  symbol             publishedDate  \\\n",
      "0    NOK  2024-06-28T16:16:07.000Z   \n",
      "1   MASI  2024-06-28T16:16:04.000Z   \n",
      "2   AAPX  2024-06-28T16:16:01.000Z   \n",
      "3    MFC  2024-06-28T16:15:59.000Z   \n",
      "4   PATH  2024-06-28T16:15:00.000Z   \n",
      "\n",
      "                                               title  \\\n",
      "0  Nokia (NOK) Set to Reboot Infrastructure Busin...   \n",
      "1  Masimo (MASI), Cleveland Clinic Unite to Enhan...   \n",
      "2  5 Leveraged ETFs That Have Gained in Double Di...   \n",
      "3  Manulife Financial (MFC) Shares Gain on New Fi...   \n",
      "4  Edelson Lechtzin LLP Urges UiPath, Inc. (NYSE:...   \n",
      "\n",
      "                                               image            site  \\\n",
      "0  https://cdn.snapi.dev/images/v1/i/a/communicat...       zacks.com   \n",
      "1  https://cdn.snapi.dev/images/v1/u/9/medical25-...       zacks.com   \n",
      "2  https://cdn.snapi.dev/images/v1/t/n/etf30-2502...       zacks.com   \n",
      "3  https://cdn.snapi.dev/images/v1/b/l/insurance9...       zacks.com   \n",
      "4  https://cdn.snapi.dev/images/v1/g/w/press15-25...  accesswire.com   \n",
      "\n",
      "                                                text  \\\n",
      "0  Nokia (NOK) is set to acquire Infinera for an ...   \n",
      "1  Masimo (MASI) collaborates with Cleveland Clin...   \n",
      "2  Wall Street has been on a solid rally in June,...   \n",
      "3  Manulife Financial (MFC) sets new financial go...   \n",
      "4  Edelson Lechtzin LLP is investigating securiti...   \n",
      "\n",
      "                                                 url sentiment  sentimentScore  \n",
      "0  https://www.zacks.com/stock/news/2295087/nokia...  Positive           0.400  \n",
      "1  https://www.zacks.com/stock/news/2295086/masim...  Positive           0.300  \n",
      "2  https://www.zacks.com/stock/news/2295088/5-lev...  Positive           0.400  \n",
      "3  https://www.zacks.com/stock/news/2295089/manul...   Neutral           0.000  \n",
      "4  https://www.accesswire.com/viewarticle.aspx?id...  Negative          -0.366  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def json_to_dataframe(json_file):\n",
    "    \"\"\"\n",
    "    Convert JSON file to a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    json_file (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame containing the JSON data.\n",
    "    \"\"\"\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Create a DataFrame from the list of articles\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "json_file = 'test.json'\n",
    "df = json_to_dataframe(json_file)\n",
    "print(df.head())  # Print the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to CSV and Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_news.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('all_news.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull Stock Price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_historical_stock_price(api_key, ticker, date_from, date_to):\n",
    "    \"\"\"\n",
    "    Fetch historical stock price data from Financial Modeling Prep API.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    ticker (str): The stock ticker symbol (e.g., 'AAPL' for Apple).\n",
    "    date_from (str): Start date for fetching data (YYYY-MM-DD).\n",
    "    date_to (str): End date for fetching data (YYYY-MM-DD).\n",
    "\n",
    "    Returns:\n",
    "    list: The parsed JSON data containing historical stock prices.\n",
    "    \"\"\"\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/historical-price-full/{ticker}?from={date_from}&to={date_to}&apikey={api_key}\"\n",
    "    return get_jsonparsed_data(url)\n",
    "\n",
    "def save_historical_prices_to_file(all_prices, filename):\n",
    "    \"\"\"\n",
    "    Save all historical stock prices to a single JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    all_prices (list): The list of all historical stock prices to save.\n",
    "    filename (str): The name of the file to save the historical stock prices.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(all_prices, file, indent=4)\n",
    "    print(f\"All historical stock prices saved to {filename}\")\n",
    "\n",
    "def fetch_and_save_all_historical_prices(api_key, tickers, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch and save historical stock prices for a list of tickers into a single JSON file over a specified timeframe.\n",
    "\n",
    "    Parameters:\n",
    "    api_key (str): Your API key for Financial Modeling Prep.\n",
    "    tickers (list): A list of ticker symbols.\n",
    "    start_date (str): Start date for fetching data (YYYY-MM-DD).\n",
    "    end_date (str): End date for fetching data (YYYY-MM-DD).\n",
    "    \"\"\"\n",
    "    all_historical_prices = []\n",
    "    for ticker in tickers:\n",
    "        print(f\"Fetching historical prices for {ticker} from {start_date} to {end_date}...\")\n",
    "        historical_prices = fetch_historical_stock_price(api_key, ticker, start_date, end_date)\n",
    "        if historical_prices:\n",
    "            all_historical_prices.append({ticker: historical_prices})\n",
    "        time.sleep(1)  # Sleep to avoid hitting API rate limits\n",
    "\n",
    "    save_historical_prices_to_file(all_historical_prices, 'all_historical_prices.json')\n",
    "\n",
    "def fetch_sp500_tickers():\n",
    "    \"\"\"\n",
    "    Fetch the list of S&P 500 ticker symbols from Wikipedia.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of S&P 500 ticker symbols.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data from Wikipedia: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', {'id': 'constituents'})\n",
    "    tickers = []\n",
    "\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        ticker = row.find_all('td')[0].text.strip()\n",
    "        tickers.append(ticker)\n",
    "\n",
    "    return tickers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching historical prices for MMM from 2023-01-01 to 2023-12-31...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j2/skjc081x2h5cd717r89c0q480000gn/T/ipykernel_80729/34511153.py:19: DeprecationWarning: cafile, capath and cadefault are deprecated, use a custom context instead.\n",
      "  response = urlopen(url, cafile=certifi.where())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching historical prices for AOS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ABT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ABBV from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ACN from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ADBE from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AMD from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AES from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AFL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for A from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for APD from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ABNB from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AKAM from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ALB from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ARE from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ALGN from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ALLE from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for LNT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ALL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for GOOGL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for GOOG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MO from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AMZN from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AMCR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AEE from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AAL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AEP from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AXP from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AIG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AMT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AWK from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AMP from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AME from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AMGN from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for APH from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ADI from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ANSS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AON from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for APA from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AAPL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AMAT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for APTV from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ACGL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ADM from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ANET from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AJG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AIZ from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for T from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ATO from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ADSK from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ADP from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AZO from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AVB from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AVY from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AXON from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BKR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BALL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BAC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BK from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BBWI from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BAX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BDX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BRK.B from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BBY from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BIO from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TECH from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BIIB from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BLK from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BA from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BKNG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BWA from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BXP from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BSX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BMY from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for AVGO from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BRO from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BF.B from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BLDR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CDNS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CZR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CPT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CPB from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for COF from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CAH from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for KMX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CCL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CARR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CTLT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CAT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CBOE from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CBRE from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CDW from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CE from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for COR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CNC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CNP from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CF from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CHRW from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CRL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for SCHW from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CHTR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CVX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CMG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CB from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CHD from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CI from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CINF from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CTAS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CSCO from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for C from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CFG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CLX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CME from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CMS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for KO from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CTSH from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CMCSA from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CAG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for COP from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ED from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for STZ from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CEG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for COO from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CPRT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for GLW from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CPAY from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CTVA from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CSGP from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for COST from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CTRA from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CRWD from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CCI from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CSX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CMI from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CVS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DHR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DRI from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DVA from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DAY from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DECK from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DE from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DAL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DVN from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DXCM from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for FANG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DLR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DFS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DLTR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for D from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DPZ from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DOV from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DOW from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DHI from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DTE from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DUK from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DD from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for EMN from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ETN from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for EBAY from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ECL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for EIX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for EW from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for EA from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ELV from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for LLY from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for EMR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ENPH from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ETR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for EOG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for EPAM from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for EQT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for EFX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for EQIX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for EQR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ESS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for EL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ETSY from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for EG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for EVRG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ES from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for EXC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for EXPE from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for EXPD from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for EXR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for XOM from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for FFIV from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for FDS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for FICO from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for FAST from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for FRT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for FDX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for FIS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for FITB from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for FSLR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for FE from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for FI from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for FMC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for F from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for FTNT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for FTV from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for FOXA from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for FOX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for BEN from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for FCX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for GRMN from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for IT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for GE from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for GEHC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for GEV from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for GEN from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for GNRC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for GD from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for GIS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for GM from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for GPC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for GILD from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for GPN from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for GL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for GDDY from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for GS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for HAL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for HIG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for HAS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for HCA from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DOC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for HSIC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for HSY from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for HES from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for HPE from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for HLT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for HOLX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for HD from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for HON from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for HRL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for HST from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for HWM from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for HPQ from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for HUBB from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for HUM from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for HBAN from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for HII from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for IBM from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for IEX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for IDXX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ITW from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for INCY from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for IR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PODD from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for INTC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ICE from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for IFF from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for IP from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for IPG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for INTU from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ISRG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for IVZ from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for INVH from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for IQV from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for IRM from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for JBHT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for JBL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for JKHY from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for J from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for JNJ from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for JCI from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for JPM from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for JNPR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for K from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for KVUE from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for KDP from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for KEY from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for KEYS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for KMB from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for KIM from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for KMI from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for KKR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for KLAC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for KHC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for KR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for LHX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for LH from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for LRCX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for LW from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for LVS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for LDOS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for LEN from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for LIN from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for LYV from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for LKQ from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for LMT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for L from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for LOW from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for LULU from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for LYB from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MTB from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MRO from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MPC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MKTX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MAR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MMC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MLM from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MAS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MA from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MTCH from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MKC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MCD from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MCK from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MDT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MRK from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for META from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MET from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MTD from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MGM from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MCHP from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MU from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MSFT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MAA from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MRNA from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MHK from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MOH from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TAP from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MDLZ from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MPWR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MNST from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MCO from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MOS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MSI from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for MSCI from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for NDAQ from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for NTAP from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for NFLX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for NEM from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for NWSA from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for NWS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for NEE from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for NKE from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for NI from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for NDSN from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for NSC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for NTRS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for NOC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for NCLH from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for NRG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for NUE from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for NVDA from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for NVR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for NXPI from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ORLY from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for OXY from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ODFL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for OMC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ON from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for OKE from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ORCL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for OTIS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PCAR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PKG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PANW from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PARA from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PH from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PAYX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PAYC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PYPL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PNR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PEP from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PFE from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PCG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PM from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PSX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PNW from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PNC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for POOL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PPG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PPL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PFG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PGR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PLD from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PRU from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PEG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PTC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PSA from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PHM from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for QRVO from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for PWR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for QCOM from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DGX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for RL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for RJF from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for RTX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for O from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for REG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for REGN from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for RF from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for RSG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for RMD from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for RVTY from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ROK from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ROL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ROP from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ROST from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for RCL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for SPGI from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for CRM from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for SBAC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for SLB from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for STX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for SRE from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for NOW from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for SHW from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for SPG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for SWKS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for SJM from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for SNA from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for SOLV from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for SO from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for LUV from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for SWK from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for SBUX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for STT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for STLD from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for STE from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for SYK from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for SMCI from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for SYF from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for SNPS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for SYY from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TMUS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TROW from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TTWO from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TPR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TRGP from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TGT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TEL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TDY from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TFX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TER from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TSLA from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TXN from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TXT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TMO from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TJX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TSCO from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TDG from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TRV from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TRMB from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TFC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TYL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for TSN from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for USB from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for UBER from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for UDR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ULTA from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for UNP from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for UAL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for UPS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for URI from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for UNH from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for UHS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for VLO from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for VTR from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for VLTO from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for VRSN from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for VRSK from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for VZ from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for VRTX from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for VTRS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for VICI from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for V from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for VST from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for VMC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for WRB from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for GWW from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for WAB from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for WBA from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for WMT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for DIS from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for WBD from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for WM from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for WAT from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for WEC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for WFC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for WELL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for WST from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for WDC from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for WRK from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for WY from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for WMB from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for WTW from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for WYNN from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for XEL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for XYL from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for YUM from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ZBRA from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ZBH from 2023-01-01 to 2023-12-31...\n",
      "Fetching historical prices for ZTS from 2023-01-01 to 2023-12-31...\n",
      "All historical stock prices saved to all_historical_prices.json\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "api_key = '28a3eef526c43ab5888ab02222aada18'\n",
    "sp500_tickers = fetch_sp500_tickers()\n",
    "\n",
    "# Limit to the first ticker (MMM)\n",
    "limited_tickers = sp500_tickers #[:1]\n",
    "\n",
    "# Specify the date range\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-12-31'\n",
    "\n",
    "if limited_tickers:\n",
    "    fetch_and_save_all_historical_prices(api_key, limited_tickers, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert prices json data to pandas dataframe, csv, parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in DataFrame: Index(['date', 'open', 'high', 'low', 'close', 'adjClose', 'volume',\n",
      "       'unadjustedVolume', 'change', 'changePercent', 'vwap', 'label',\n",
      "       'changeOverTime', 'ticker'],\n",
      "      dtype='object')\n",
      "Data saved to all_historical_prices.csv\n",
      "Data saved to all_historical_prices.parquet\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def json_to_dataframe(json_file):\n",
    "    \"\"\"\n",
    "    Convert JSON file to a pandas DataFrame in tidy data format.\n",
    "\n",
    "    Parameters:\n",
    "    json_file (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame in tidy data format.\n",
    "    \"\"\"\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    records = []\n",
    "    for ticker_data in data:\n",
    "        for ticker, content in ticker_data.items():\n",
    "            historical_data = content[\"historical\"]\n",
    "            for price in historical_data:\n",
    "                price['ticker'] = ticker\n",
    "                records.append(price)\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "def save_dataframe_to_csv(df, csv_file):\n",
    "    \"\"\"\n",
    "    Save DataFrame to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The pandas DataFrame to save.\n",
    "    csv_file (str): The name of the CSV file.\n",
    "    \"\"\"\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"Data saved to {csv_file}\")\n",
    "\n",
    "def save_dataframe_to_parquet(df, parquet_file):\n",
    "    \"\"\"\n",
    "    Save DataFrame to a Parquet file.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The pandas DataFrame to save.\n",
    "    parquet_file (str): The name of the Parquet file.\n",
    "    \"\"\"\n",
    "    df.to_parquet(parquet_file, index=False)\n",
    "    print(f\"Data saved to {parquet_file}\")\n",
    "\n",
    "# Example usage\n",
    "json_file = 'all_historical_prices.json'\n",
    "csv_file = 'all_historical_prices.csv'\n",
    "parquet_file = 'all_historical_prices.parquet'\n",
    "\n",
    "# Convert JSON to DataFrame\n",
    "df = json_to_dataframe(json_file)\n",
    "\n",
    "# Check if all columns are retained\n",
    "print(\"Columns in DataFrame:\", df.columns)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "save_dataframe_to_csv(df, csv_file)\n",
    "\n",
    "# Save DataFrame to Parquet\n",
    "save_dataframe_to_parquet(df, parquet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py31",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
