─    ~/daen_programming/ait526/ait526-teamprojects/Main-Project    mainproj-tweak-marcos *4 !1 ?1 
╰─ python test_data.py                                                                                   ─╯
2024-07-01 07:18:12,928 - Loading sentiment analysis model...
No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).
Using a pipeline without specifying a model name and revision in production is not recommended.
2024-07-01 07:18:13,724 - Processing data in 10 batches...
2024-07-01 07:18:13,724 - Processing batch 1/10...
2024-07-01 07:19:26,322 - Processing batch 2/10...
2024-07-01 07:20:35,868 - Processing batch 3/10...
2024-07-01 07:21:46,421 - Processing batch 4/10...
2024-07-01 07:22:52,721 - Processing batch 5/10...
2024-07-01 07:24:01,800 - Processing batch 6/10...
2024-07-01 07:25:09,826 - Processing batch 7/10...
2024-07-01 07:26:16,860 - Processing batch 8/10...
2024-07-01 07:27:28,201 - Processing batch 9/10...
2024-07-01 07:28:35,998 - Processing batch 10/10...
2024-07-01 07:29:48,571 - Total dataset size: 9999 rows
total dataset size: 9999 rows
2024-07-01 07:29:48,593 - Overlap between training and test data: 203 articles
overlap between training and test data: 203 articles
training data sentiment distribution:
sentiment
2    4065
1    1972
0    1962
Name: count, dtype: int64

test data sentiment distribution:
sentiment
2    1016
1     493
0     491
Name: count, dtype: int64
2024-07-01 07:29:48,596 - Training data sentiment distribution:
2024-07-01 07:29:48,596 - sentiment
2    4065
1    1972
0    1962
Name: count, dtype: int64
2024-07-01 07:29:48,597 -
Test data sentiment distribution:
2024-07-01 07:29:48,597 - sentiment
2    1016
1     493
0     491
Name: count, dtype: int64
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/opt/anaconda3/envs/py31/lib/python3.11/site-packages/threadpoolctl.py:1010: RuntimeWarning:
Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at
the same time. Both libraries are known to be incompatible and this
can cause random crashes or deadlocks on Linux when loaded in the
same Python program.
Using threadpoolctl may cause crashes or deadlocks. For more
information and possible workarounds, please see
    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md

  warnings.warn(msg, RuntimeWarning)
naive bayes model cross-validation results:
accuracy     0.748175
precision    0.755637
recall       0.748175
f1_score     0.748669
dtype: float64
2024-07-01 07:29:49,999 - Naive Bayes model cross-validation results:
2024-07-01 07:29:50,000 - accuracy     0.748175
precision    0.755637
recall       0.748175
f1_score     0.748669
dtype: float64
naive bayes model evaluation on test data:
              precision    recall  f1-score   support

           0       0.80      0.70      0.75       491
           1       0.57      0.52      0.54       493
           2       0.75      0.83      0.79      1016

    accuracy                           0.72      2000
   macro avg       0.71      0.68      0.69      2000
weighted avg       0.72      0.72      0.72      2000

accuracy: 0.7205
precision: 0.7183859956178045
recall: 0.7205
f1 score: 0.7174946783939021
2024-07-01 07:29:50,070 - Naive Bayes model evaluation on test data:
2024-07-01 07:29:50,075 -               precision    recall  f1-score   support

           0       0.80      0.70      0.75       491
           1       0.57      0.52      0.54       493
           2       0.75      0.83      0.79      1016

    accuracy                           0.72      2000
   macro avg       0.71      0.68      0.69      2000
weighted avg       0.72      0.72      0.72      2000

2024-07-01 07:29:50,075 - Accuracy: 0.7205
2024-07-01 07:29:50,075 - Precision: 0.7183859956178045
2024-07-01 07:29:50,075 - Recall: 0.7205
2024-07-01 07:29:50,075 - F1 score: 0.7174946783939021
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
tokenizer_config.json: 100%|█████████████████████████████████████████████| 48.0/48.0 [00:00<00:00, 54.8kB/s]
vocab.txt: 100%|█████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 2.86MB/s]
tokenizer.json: 100%|████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 14.9MB/s]
config.json: 100%|██████████████████████████████████████████████████████████| 570/570 [00:00<00:00, 847kB/s]
model.safetensors: 100%|█████████████████████████████████████████████████| 440M/440M [00:04<00:00, 93.0MB/s]
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.746, 'learning_rate': 5e-05, 'epoch': 0.5}
{'loss': 0.5634, 'learning_rate': 4e-05, 'epoch': 1.0}
{'loss': 0.4027, 'learning_rate': 3e-05, 'epoch': 1.5}
{'loss': 0.3797, 'learning_rate': 2e-05, 'epoch': 2.0}
{'loss': 0.2253, 'learning_rate': 1e-05, 'epoch': 2.5}
{'loss': 0.2466, 'learning_rate': 0.0, 'epoch': 3.0}
{'train_runtime': 2394.2489, 'train_samples_per_second': 10.023, 'train_steps_per_second': 1.253, 'train_loss': 0.4273019790649414, 'epoch': 3.0}
100%|███████████████████████████████████████████████████████████████████| 3000/3000 [39:54<00:00,  1.25it/s]
2024-07-01 08:09:58,772 - BERT model evaluation:
bert model evaluation:
100%|█████████████████████████████████████████████████████████████████████| 250/250 [00:47<00:00,  5.27it/s]
100%|█████████████████████████████████████████████████████████████████████| 250/250 [00:50<00:00,  4.99it/s]
bert model evaluation on test data:
              precision    recall  f1-score   support

           0       0.94      0.86      0.90       491
           1       0.70      0.74      0.72       493
           2       0.89      0.90      0.90      1016

    accuracy                           0.85      2000
   macro avg       0.84      0.84      0.84      2000
weighted avg       0.86      0.85      0.85      2000

accuracy: 0.8535
precision: 0.8569799207214847
recall: 0.8535
f1 score: 0.8547352961688798
2024-07-01 08:11:36,592 - BERT model evaluation on test data:
2024-07-01 08:11:36,597 -               precision    recall  f1-score   support

           0       0.94      0.86      0.90       491
           1       0.70      0.74      0.72       493
           2       0.89      0.90      0.90      1016

    accuracy                           0.85      2000
   macro avg       0.84      0.84      0.84      2000
weighted avg       0.86      0.85      0.85      2000
